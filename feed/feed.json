{
	"version": "https://jsonfeed.org/version/1.1",
	"title": "dctanner&#39;s blog",
	"language": "en",
	"home_page_url": "https://dctanner.github.io/",
	"feed_url": "https://dctanner.github.io/feed/feed.json",
	"description": "Notes from a journey of compounding curiosity.",
	"author": {
		"name": "Damien Tanner",
		"url": ""
	},
	"items": [
		{
			"id": "https://dctanner.github.io/blog/your-rag-may-not-need-a-vector-store/",
			"url": "https://dctanner.github.io/blog/your-rag-may-not-need-a-vector-store/",
			"title": "Using LLM tool calling and long context for better RAG",
			"content_html": "<p>When building a RAG pipeline you'll probably reach for a vector store to store embeddings of document chunks, which are then retrieved and put into context at query time. This works well if your users are asking single fact queries where the answer can be found in a relevant document chunk. But if you just want to ask more complex questions where the answer requires information, spread across the whole document or across multiple documents, retreiveing chunks often leaves out critical information and can lead to inaccurate responses.</p>\n<p>Relying on document chunks has been a great solution to add knowledge to LLMs with limited context windows. But context windows have grown massively over the past year, with the leading LLMs supporting over 200,000 token context windows. This opens the door to new approaches to RAG which are less constrained by context.</p>\n<h2 id=\"whole-document-querying-rag\" tabindex=\"-1\">Whole document querying RAG <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/your-rag-may-not-need-a-vector-store/\">#</a></h2>\n<p>Instead of retrieving document chunks, I've had success retreiving and querying whole documents. The result is that all the knowledge in a document is included. Queries like 'summarize xyz' yield a full and complete summary without risk of missing important details.</p>\n<p>When does this work? This approach works best if your documents are all of the same type or can be put into categories. It works especially well if the user queries include enough information to locate the specific document(s) the question is for.or</p>\n<p>For example, if your documents are client contracts, each may have a client name, date and contract type. If a user asks 'Summarize the most recent contract with Acme Inc?' we have enough information to find this document, and then use the whole document as context to fully answer their request.</p>\n<p>Querying whole documents like this calls for a different RAG workflow than the common single step chunk-retrieve-query workflow. Retrieving whole documents and putting them straight into the context could fill up even the largest context windows.</p>\n<p>Instead, we can leverage the function/tool calling ability of many LLMs to create sub-queries to query each document. We can even make use of cheaper and faster LLMs for these sub-queries which have to process the complete documents.</p>\n<p>What does this look like in practice?</p>\n<h3 id=\"create-document-query-functions\" tabindex=\"-1\">Create document query functions <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/your-rag-may-not-need-a-vector-store/\">#</a></h3>\n<p>In the example above, we would need to be able to locate and query a client contract document. We can create a function which takes several search filters, retrieves the full text of the top matching document, and then calls an LLM (e.g. gpt-3.5-turbo) with the full document text and the query. The fuction should accept the filters required to find the document e.g.: client name, date range, contract type. Plus a 'query' param which is the query to send to the LLM with the full document text.</p>\n<p>There's no right way to search for these documents, you could use SQL, Elastic or even embeddings. The key thing is it should be able handle fuzzy search filters, e.g. for the client name and contract type.</p>\n<p>Here's an example of this function:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">query_client_contract</span><span class=\"token punctuation\">(</span>client_name<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> document_type<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> from_date<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> to_date<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> query<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\t<span class=\"token comment\"># Search for the document</span>\n\tdocument <span class=\"token operator\">=</span> search_client_contract<span class=\"token punctuation\">(</span>client_name<span class=\"token punctuation\">,</span> document_type<span class=\"token punctuation\">,</span> from_date<span class=\"token punctuation\">,</span> to_date<span class=\"token punctuation\">)</span>\n\t<span class=\"token comment\"># Call the LLM with the full document text and the query</span>\n    messages <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">{</span><span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Answer the query using the provided text.\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"role\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"system\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">{</span><span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> document <span class=\"token operator\">+</span> <span class=\"token string\">\"\\n\\nQuery: \"</span> <span class=\"token operator\">+</span> query<span class=\"token punctuation\">,</span> <span class=\"token string\">\"role\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"user\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">]</span>\n\tresponse <span class=\"token operator\">=</span> client<span class=\"token punctuation\">.</span>chat<span class=\"token punctuation\">.</span>completions<span class=\"token punctuation\">.</span>create<span class=\"token punctuation\">(</span>\n        model<span class=\"token operator\">=</span><span class=\"token string\">\"gpt-3.5-turbo\"</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># Use a cheaper model for the sub-query which will process the full document</span>\n        messages<span class=\"token operator\">=</span>messages<span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n\t<span class=\"token keyword\">return</span> response<span class=\"token punctuation\">.</span>choices<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>message<span class=\"token punctuation\">.</span>content</code></pre>\n<h3 id=\"sub-query-function-calls\" tabindex=\"-1\">Sub-query function calls <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/your-rag-may-not-need-a-vector-store/\">#</a></h3>\n<p>Now we have document seach function, we are going to use LLM function calling (see <a href=\"https://platform.openai.com/docs/guides/function-calling\">OpenAI Function Calling</a> for example) to create sub-queries to this document query function.</p>\n<p>Here's an json schema spec of the above tool, as required for OpenAI Function Calling:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\">tools <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n\t<span class=\"token punctuation\">{</span>\n\t\t<span class=\"token string\">\"type\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"function\"</span><span class=\"token punctuation\">,</span>\n\t\t<span class=\"token string\">\"function\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n\t\t\t<span class=\"token string\">\"name\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"query_client_contract\"</span><span class=\"token punctuation\">,</span>\n\t\t\t<span class=\"token string\">\"description\"</span><span class=\"token punctuation\">:</span>\n\t\t\t\t<span class=\"token string\">\"Send the query to AI to ask the full document text. The AI response will be returned.\"</span><span class=\"token punctuation\">,</span>\n\t\t\t<span class=\"token string\">\"parameters\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n\t\t\t\t<span class=\"token string\">\"type\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"object\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t<span class=\"token string\">\"properties\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n\t\t\t\t\t<span class=\"token string\">\"client_name\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n\t\t\t\t\t\t<span class=\"token string\">\"type\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"string\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t\t<span class=\"token string\">\"description\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Name of the client the contract is for.\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t<span class=\"token string\">\"document_type\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n\t\t\t\t\t\t<span class=\"token string\">\"type\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"string\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t\t<span class=\"token string\">\"enum\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"contract\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"lease\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t\t<span class=\"token string\">\"description\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"The type of legal contract.\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t<span class=\"token string\">\"from_date\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n\t\t\t\t\t\t<span class=\"token string\">\"type\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"string\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t\t<span class=\"token string\">\"format\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"date-time\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t\t<span class=\"token string\">\"description\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Find documents from this date.\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t<span class=\"token string\">\"to_date\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n\t\t\t\t\t\t<span class=\"token string\">\"type\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"string\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t\t<span class=\"token string\">\"format\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"date-time\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t\t<span class=\"token string\">'description'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Find documents up to this date.\"</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n\t\t\t\t<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n\t\t\t\t<span class=\"token string\">\"required\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"client_name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"document_type\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n\t\t\t<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n\t\t<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n\t<span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">]</span></code></pre>\n<p>Then we need can create a helper function to execute the function when requested by the LLM:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">execute_function_call</span><span class=\"token punctuation\">(</span>message<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> message<span class=\"token punctuation\">.</span>tool_calls<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>function<span class=\"token punctuation\">.</span>name <span class=\"token operator\">==</span> <span class=\"token string\">\"query_client_contract\"</span><span class=\"token punctuation\">:</span>\n        args <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>loads<span class=\"token punctuation\">(</span>message<span class=\"token punctuation\">.</span>tool_calls<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>function<span class=\"token punctuation\">.</span>arguments<span class=\"token punctuation\">)</span>\n        results <span class=\"token operator\">=</span> ask_database<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">[</span><span class=\"token string\">\"client_name\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> args<span class=\"token punctuation\">[</span><span class=\"token string\">\"document_type\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> args<span class=\"token punctuation\">[</span><span class=\"token string\">\"from_date\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> args<span class=\"token punctuation\">[</span><span class=\"token string\">\"to_date\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> args<span class=\"token punctuation\">[</span><span class=\"token string\">\"query\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        results <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f\"Error: function </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>message<span class=\"token punctuation\">.</span>tool_calls<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>function<span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">}</span></span><span class=\"token string\"> does not exist\"</span></span>\n    <span class=\"token keyword\">return</span> results</code></pre>\n<p>Now in the main chat function, we take a user's query, and if GPT suggests a function call, we execute it and append the results to the chat messages, and then send the messages back to GPT for the final answer:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">ask_ai</span><span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    messages <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">{</span><span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Answer the user query, calling functions if required.\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"role\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"system\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">{</span><span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> query<span class=\"token punctuation\">,</span> <span class=\"token string\">\"role\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"user\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">]</span>\n\n\tchat_response <span class=\"token operator\">=</span> client<span class=\"token punctuation\">.</span>chat<span class=\"token punctuation\">.</span>completions<span class=\"token punctuation\">.</span>create<span class=\"token punctuation\">(</span>\n        model<span class=\"token operator\">=</span><span class=\"token string\">\"gpt-4-turbo\"</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># Use a more powerful model for function calling</span>\n        tools<span class=\"token operator\">=</span>tools<span class=\"token punctuation\">,</span>\n        tool_choice<span class=\"token operator\">=</span><span class=\"token string\">\"auto\"</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># \"auto\" means the model can pick between generating a message or calling a function</span>\n        messages<span class=\"token operator\">=</span>messages<span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n\n\tassistant_message <span class=\"token operator\">=</span> chat_response<span class=\"token punctuation\">.</span>choices<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>message\n\tassistant_message<span class=\"token punctuation\">.</span>content <span class=\"token operator\">=</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>assistant_message<span class=\"token punctuation\">.</span>tool_calls<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>function<span class=\"token punctuation\">)</span>\n\tmessages<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"role\"</span><span class=\"token punctuation\">:</span> assistant_message<span class=\"token punctuation\">.</span>role<span class=\"token punctuation\">,</span> <span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> assistant_message<span class=\"token punctuation\">.</span>content<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n\t<span class=\"token keyword\">if</span> assistant_message<span class=\"token punctuation\">.</span>tool_calls<span class=\"token punctuation\">:</span>\n\t\tresults <span class=\"token operator\">=</span> execute_function_call<span class=\"token punctuation\">(</span>assistant_message<span class=\"token punctuation\">)</span>\n\t\tmessages<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"role\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"function\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"tool_call_id\"</span><span class=\"token punctuation\">:</span> assistant_message<span class=\"token punctuation\">.</span>tool_calls<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">id</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"name\"</span><span class=\"token punctuation\">:</span> assistant_message<span class=\"token punctuation\">.</span>tool_calls<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>function<span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">,</span> <span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> results<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n\tsecond_chat_response <span class=\"token operator\">=</span> client<span class=\"token punctuation\">.</span>chat<span class=\"token punctuation\">.</span>completions<span class=\"token punctuation\">.</span>create<span class=\"token punctuation\">(</span>\n        model<span class=\"token operator\">=</span><span class=\"token string\">\"gpt-4-turbo\"</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># Use a more powerful model for function calling</span>\n        tools<span class=\"token operator\">=</span>tools<span class=\"token punctuation\">,</span>\n        tool_choice<span class=\"token operator\">=</span><span class=\"token string\">\"auto\"</span><span class=\"token punctuation\">,</span> <span class=\"token comment\"># \"auto\" means the model can pick between generating a message or calling a function</span>\n        messages<span class=\"token operator\">=</span>messages<span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n\t<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>second_chat_response<span class=\"token punctuation\">.</span>choices<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>message<span class=\"token punctuation\">.</span>content<span class=\"token punctuation\">)</span></code></pre>\n<p>There are several benefits to this approach. The main benefit, as discussed above, is that we are querying whole documents. For many use cases this is going to provide more complete answers for users. You can also easily extend this approach by adding more functions for different document types and data sources. GPT will call multiple functions which you can execute in parallel, and in the final GPT call we can use gpt-4-turbo to integrate the results and provide the final answer. If you do have a handful of unknown documents, you can still use the chunk-retrieve-query approach for those, and simply add a function to the tool list to query the chunked documents with a typical RAG pipeline.</p>\n<p>I'm excited to see how this approach can be used in practice. I think it will be especially useful for complex questions where the answer is spread across multiple documents, or where the user query is for a summary of a document. I'd love to hear how you get on with this approach. Please reach out if you have any other ideas for how to improve this approach, or related new ideas for improving RAG.</p>\n",
			"date_published": "2024-04-25T00:00:00Z"
		}
		,
		{
			"id": "https://dctanner.github.io/blog/building-an-ai-superserver/",
			"url": "https://dctanner.github.io/blog/building-an-ai-superserver/",
			"title": "Building an AI SuperServer for LLM training and experiments",
			"content_html": "<p>Impressive new language models like Llama and Mistral have broadened the accessibility of AI training. If you want to fine-tune a model with your own data, it's now relatively easy to do with tools like <a href=\"https://github.com/OpenAccess-AI-Collective/axolotl\">Axolotl</a> and a few dollars spent on a GPU cloud. But if you want to go deeper and train larger models or try new methods, the cloud bill can quickly rack up. Renting 8 A100's on AWS will set you back an astounding $350,000 per year! There are cheaper clouds, but they can still cost tens of thousands a year.</p>\n<p>I've always enjoyed building PCs. I remember when I was 16 and my grandma bought me my first PC to assemble myself. So in the name of fun and saving money, I embarked on building an AI server so that I can more affordably do independent AI research.</p>\n<h1 id=\"your-options\" tabindex=\"-1\">Your options <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h1>\n<p>Depending on your budget and use case, there are a few routes to take when building an AI server.</p>\n<h2 id=\"open-frame\" tabindex=\"-1\">Open frame <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h2>\n<p><picture><source type=\"image/avif\" srcset=\"https://dctanner.github.io/img/l6X6xCxgFa-2048.avif 2048w\"><source type=\"image/webp\" srcset=\"https://dctanner.github.io/img/l6X6xCxgFa-2048.webp 2048w\"><img alt=\"Miner style\" loading=\"lazy\" decoding=\"async\" style=\"width: 100%; height: auto;\" src=\"https://dctanner.github.io/img/l6X6xCxgFa-2048.jpeg\" width=\"2048\" height=\"1536\"></picture></p>\n<p>If the server is just for you, and you want to keep it at home or in your basement, the most affordable option is essentially a powerful consumer PC, with an open frame case (originally designed for crypto miners). You'll be able to find a lots of advice on Reddit for this route.</p>\n<p>The important things are a motherboard that has lots of 16x PCIe slots, PCIe risers with redrivers, and multiple PSUs (depending the number of GPUs you choose). You'll be able to buy everything second had if you like, including the GPUs. For GPUs you're best going with RTX 3090s or 4090s in this setup, and because there's no case, you won't have issues with space or airflow.</p>\n<p>The benefit if this route is cost, but also the ability to start simple with just a single GPU and grow as you desire by adding more.</p>\n<h2 id=\"rack-server\" tabindex=\"-1\">Rack server <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h2>\n<p><picture><source type=\"image/avif\" srcset=\"https://dctanner.github.io/img/U4DUeJHjp3-1280.avif 1280w\"><source type=\"image/webp\" srcset=\"https://dctanner.github.io/img/U4DUeJHjp3-1280.webp 1280w\"><img alt=\"Server style\" loading=\"lazy\" decoding=\"async\" style=\"width: 100%; height: auto;\" src=\"https://dctanner.github.io/img/U4DUeJHjp3-1280.jpeg\" width=\"1280\" height=\"960\"></picture></p>\n<p>If you're planning to train larger models, have more servers, datacenter GPUs or just don't have anywhere to house a noisy hot server at home, you can go the rack mountable server route. This is the the route I've gone, as our house doesn't have a basement and our internet isn't that fast. My server now lives in a datacenter where it's cooled and well connected.</p>\n<p>I found less resources on this route, so the rest of this guide is aimed at helping you build and setup a rack mountable GPU server.</p>\n<h1 id=\"building-the-server\" tabindex=\"-1\">Building the server <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h1>\n<p>Supermicro make great server systems and many specifically for AI use cases. For example the <a href=\"https://www.supermicro.com/en/products/system/4u/4029/sys-4029gp-trt2.cfm\">SuperServer 4029GP-TRT2</a> is a mid range 4U dual CPU server with 10 PCIe slots - ideal for filling with GPUs! I found a well priced one from an IT supplier in the UK. The newer model is more expensive, but may be easier to find. Note that the model I used only have PCIe 3.0. If you are using RTX 4090 or a newer datacenter GPU, you will probably want the newer model which supports PCIe 4.0.</p>\n<p><picture><source type=\"image/avif\" srcset=\"https://dctanner.github.io/img/rKkyq27rxN-960.avif 960w\"><img alt=\"SuperServer 4029GP-TRT2\" loading=\"lazy\" decoding=\"async\" style=\"width: 100%; height: auto;\" src=\"https://dctanner.github.io/img/rKkyq27rxN-960.webp\" width=\"960\" height=\"720\"></picture></p>\n<p>It arrived at my house on a pallet. It was heavier than I expected!</p>\n<p><picture><source type=\"image/avif\" srcset=\"https://dctanner.github.io/img/x5qFkxTOGi-1280.avif 1280w\"><source type=\"image/webp\" srcset=\"https://dctanner.github.io/img/x5qFkxTOGi-1280.webp 1280w\"><img alt=\"The pallet\" loading=\"lazy\" decoding=\"async\" style=\"width: 100%; height: auto;\" src=\"https://dctanner.github.io/img/x5qFkxTOGi-1280.jpeg\" width=\"1280\" height=\"960\"></picture></p>\n<p>After lugging it up the stairs and reading the manual, I installed 10 RTX 3090s I bought second hand from someone who previously was using them for mining. Note that to fit the maximum number of GPUs in a system you'll need to find blower or turbo style GPUs that are only two slots wide. The vast majority of 3090 and 4090 GPUs are for gaming, and they will take up 3 slots and the power comes out the top and you won't be able to put the case on your server. If you can't find blower consumer GPUs, you're next best bet is the RTX A6000 which is still fairly good value for money, even if it's still 3x more than a 4090.</p>\n<p>You'll also need to add the CPUs (two of them), memory and storage. I sourced everything secondhand from eBay. Most things cost no more than a few hundred dollars each. I went with 2x Intel Xeon Platinum 8160, 12x32GB DDR memory and an 8TB SSD.</p>\n<p>Once everything was installed, I turned it on for the first time - what I heard could only be described as a mini jet engine. Server fans are noisy.</p>\n<p>Next step was to setup the OS and environment.</p>\n<h1 id=\"setting-up-the-os\" tabindex=\"-1\">Setting up the OS <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h1>\n<p>Supermicro servers have in inbuilt webui called IPMI for accessing the server console and monitor output. There is a dedicated lan port for PICE on this server. You should also plug in a second lan cable to one of the main lan ports, otherwise your server won't actually have internet access (this confused me initially).</p>\n<p>It will find an IP with DHCP, so I just logged into my router to see the IP it was assigned and visited that in my browser. You'll be asked to login, username is 'ADMIN' and the password is printed stickers in several places in your server case.</p>\n<p>I decided to install Ubuntu 22.04 sever. Create a bootable Ubuntu USB stick and plug it into the server. Now connect to the webui console by going to the server's IP then clicking Remote Control &gt; iKVM/HTML5 and click the button. You can now reboot the server and you'll see the BIOS popup, where you can hit an F key to choose a boot drive. Do this and select the USB.</p>\n<p>The IPMI web console doesn't support pasting text. So getting your ssh pubkey over is a bit of a pain. Here's a solution I've used:</p>\n<ol>\n<li>On your local computer with has your ssh pubkey on it, run <code>cd .ssh &amp;&amp; python -m http.server</code> (you are about to serve your private key over http without authentication, please be aware this isn't a great idea).</li>\n<li>On the server, via the IPMI web console, login with the user you created when installing Ubuntu, and run <code>wget -qO - &quot;http://192.168.178.21:8000/id_ed25519.pub&quot; &gt; ~/.ssh/authorized_keys &amp;&amp; chmod 600 .ssh/authorized_keys</code>.</li>\n<li>You should now be able to ssh into your server. Remember to stop the <code>python -m http.server</code> on your local computer now.</li>\n</ol>\n<h1 id=\"important-system-tweaks\" tabindex=\"-1\">Important system tweaks <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h1>\n<p>There are some tweaks we can do to improve the performance and reliability of our server. Following <a href=\"https://towardsdatascience.com/deploying-kubeflow-to-a-bare-metal-gpu-cluster-from-scratch-6865ebcde032\">the tips here</a> (<a href=\"https://archive.ph/0Y2DK#selection-611.0-611.103\">archived page</a> if Medium paywalls that page), first disable the kernel security patches on computing instances. The collateral performance penalty is much more expensive than the imposed risks. Edit /etc/default/grub and add:</p>\n<pre><code>GRUB_CMDLINE_LINUX_DEFAULT=&quot;pti=off spectre_v2=off l1tf=off nospec_store_bypass_disable no_stf_barrier&quot;\n</code></pre>\n<p>It's also critical to disable IOMMU if you plan peer-to-peer GPU communication, e.g., multi-GPU model training in Tensorflow or PyTorch. Also add to /etc/default/grub:</p>\n<pre><code>GRUB_CMDLINE_LINUX_DEFAULT=&quot;intel_iommu=off rcutree.rcu_idle_gp_delay=1&quot;\n</code></pre>\n<h1 id=\"check-gpu-p2p-communication\" tabindex=\"-1\">Check GPU P2P communication <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h1>\n<p>If you're using a GPU that supports it, P2P communication speeds up things a lot.</p>\n<p>Note it's important check <a href=\"https://docs.nvidia.com/deeplearning/nccl/archives/nccl_284/user-guide/docs/troubleshooting.html#:~:text=PCI%20Access%20Control%20Services%20(ACS)%C2%B6&amp;text=If%20PCI%20switches%20have%20ACS,done%20again%20after%25z\">PCI Access Control Services (ACS)</a> is disabled.</p>\n<p>You can follow these steps to test your system's GPU P2P speed: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#gpu-to-gpu-communication</p>\n<h1 id=\"nvidia-drivers-and-python-environment\" tabindex=\"-1\">NVIDIA drivers and python environment <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h1>\n<p>We now want to get the NVIDIA drivers, CUDA and our Python envs setup.</p>\n<p>I've had success using these steps to install CUDA v11.8: https://gist.github.com/MihailCosmin/affa6b1b71b43787e9228c25fe15aeba\nSome people have mentioned using a higher NVIDIA drivers version than the nvidia-driver-515 in the script. But be beware there's a bug in driver version 545 that prevents 3090 and 4090 cards from using P2P (see <a href=\"https://github.com/NVIDIA/nccl-tests/issues/117\">this github issue</a> for a discussion on the problem). If you have a driver with this bug, you may find your training run stalls and times out. Version 535 worked well for me.</p>\n<p>I like to use Conda with the <a href=\"https://www.fast.ai/posts/2021-07-15-fastconda.html\">fastchan channel</a> for my environments. But you may enjoy a different python virtual env tool.</p>\n<h1 id=\"now-you-can-train-some-ai\" tabindex=\"-1\">Now you can train some AI <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h1>\n<p><picture><source type=\"image/avif\" srcset=\"https://dctanner.github.io/img/WfDXmhx_Ty-1482.avif 1482w\"><source type=\"image/webp\" srcset=\"https://dctanner.github.io/img/WfDXmhx_Ty-1482.webp 1482w\"><img alt=\"nvidia-smi\" loading=\"lazy\" decoding=\"async\" style=\"width: 100%; height: auto;\" src=\"https://dctanner.github.io/img/WfDXmhx_Ty-1482.jpeg\" width=\"1482\" height=\"1338\"></picture></p>\n<p>I'm enjoying using <a href=\"https://github.com/OpenAccess-AI-Collective/axolotl\">Axolotl</a> for LLM fine tuning. <a href=\"https://huggingface.co/docs/transformers/index\">HuggingFace Transformers</a> is also a great place to start.</p>\n<h1 id=\"datacenter-trip\" tabindex=\"-1\">Datacenter trip <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h1>\n<p><picture><source type=\"image/avif\" srcset=\"https://dctanner.github.io/img/Fd4oi2rcxB-1922.avif 1922w\"><source type=\"image/webp\" srcset=\"https://dctanner.github.io/img/Fd4oi2rcxB-1922.webp 1922w\"><img alt=\"Datacenter\" loading=\"lazy\" decoding=\"async\" style=\"width: 100%; height: auto;\" src=\"https://dctanner.github.io/img/Fd4oi2rcxB-1922.png\" width=\"1922\" height=\"1294\"></picture></p>\n<p>Since the GPUs are super noisy and hot, I found a local datacenter that would colocate it for a reasonable cost. Installation was easier than I expected, although we ended up putting it on a lower slot on the rack because it was too heavy to lift half way up without a lift.</p>\n<p>This <a href=\"https://www.datacate.net/wp-content/uploads/2019/04/Colocation-Survival-Guide-6x9-with-bonus-material.pdf\">Colocation Survival Guide</a> was super helpful, as it walks you through all the aspects of colocating, from the physical setup to networking.</p>\n<h1 id=\"other-things\" tabindex=\"-1\">Other things <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h1>\n<h2 id=\"set-a-lower-max-power-limit-for-gpus\" tabindex=\"-1\">Set a lower max power limit for GPUs <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h2>\n<p>Some people find that lowering the power limit just a bit will reduce max temp without any real performance sacrifice. I set the max power for my RTX 3090's to 300W (from 305W) by <a href=\"https://www.reddit.com/r/Fedora/comments/11lh9nn/set_nvidia_gpu_power_and_temp_limit_on_boot/\">following these steps</a>.</p>\n<h2 id=\"docker-bug-workaround\" tabindex=\"-1\">Docker bug workaround <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h2>\n<p>If you're planning to use Docker with the GPUs, note there's <a href=\"https://github.com/NVIDIA/nvidia-container-toolkit/issues/48\">a bug on Ubuntu 22.04 which needs working around</a>.</p>\n<h1 id=\"going-bigger\" tabindex=\"-1\">Going bigger? <a class=\"header-anchor\" href=\"https://dctanner.github.io/blog/building-an-ai-superserver/\">#</a></h1>\n<p>If you're planing to build a cluster, there is an excellent video from the Lambda team: <a href=\"https://www.youtube.com/watch?v=rfu5FwncZ6s\">Building a GPU cluster for AI</a>.</p>\n",
			"date_published": "2024-03-14T00:00:00Z"
		}
		
	]
}
