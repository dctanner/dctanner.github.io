<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>Building an AI SuperServer for LLM training and experiments</title>
		<meta name="description" content="Notes from a journey of compounding curiosity.">
		<link rel="alternate" href="/feed/feed.xml" type="application/atom+xml" title="dctanner&#39;s blog">
		<link rel="alternate" href="/feed/feed.json" type="application/json" title="dctanner&#39;s blog">
		
		<style>/**
 * okaidia theme for JavaScript, CSS and HTML
 * Loosely based on Monokai textmate theme by http://www.monokai.nl/
 * @author ocodia
 */

code[class*="language-"],
pre[class*="language-"] {
	color: #f8f8f2;
	background: none;
	text-shadow: 0 1px rgba(0, 0, 0, 0.3);
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	font-size: 1em;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
	border-radius: 0.3em;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #272822;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: #8292a2;
}

.token.punctuation {
	color: #f8f8f2;
}

.token.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.constant,
.token.symbol,
.token.deleted {
	color: #f92672;
}

.token.boolean,
.token.number {
	color: #ae81ff;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #a6e22e;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string,
.token.variable {
	color: #f8f8f2;
}

.token.atrule,
.token.attr-value,
.token.function,
.token.class-name {
	color: #e6db74;
}

.token.keyword {
	color: #66d9ef;
}

.token.regex,
.token.important {
	color: #fd971f;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
/*
 * New diff- syntax
 */

pre[class*="language-diff-"] {
	--eleventy-code-padding: 1.25em;
	padding-left: var(--eleventy-code-padding);
	padding-right: var(--eleventy-code-padding);
}
.token.deleted {
	background-color: hsl(0, 51%, 37%);
	color: inherit;
}
.token.inserted {
	background-color: hsl(126, 31%, 39%);
	color: inherit;
}

/* Make the + and - characters unselectable for copy/paste */
.token.prefix.unchanged,
.token.prefix.inserted,
.token.prefix.deleted {
	-webkit-user-select: none;
	user-select: none;
	display: inline-flex;
	align-items: center;
	justify-content: center;
	padding-top: 2px;
	padding-bottom: 2px;
}
.token.prefix.inserted,
.token.prefix.deleted {
	width: var(--eleventy-code-padding);
	background-color: rgba(0,0,0,.2);
}

/* Optional: full-width background color */
.token.inserted:not(.prefix),
.token.deleted:not(.prefix) {
	display: block;
	margin-left: calc(-1 * var(--eleventy-code-padding));
	margin-right: calc(-1 * var(--eleventy-code-padding));
	text-decoration: none; /* override del, ins, mark defaults */
	color: inherit; /* override del, ins, mark defaults */
}
* { box-sizing: border-box; }
/* Defaults */
:root {
	--font-family: -apple-system, system-ui, sans-serif;
	--font-family-monospace: Consolas, Menlo, Monaco, Andale Mono WT, Andale Mono, Lucida Console, Lucida Sans Typewriter, DejaVu Sans Mono, Bitstream Vera Sans Mono, Liberation Mono, Nimbus Mono L, Courier New, Courier, monospace;
}

/* Theme colors */
:root {
	--color-gray-20: #e0e0e0;
	--color-gray-50: #C0C0C0;
	--color-gray-90: #333;

	--background-color: #fff;

	--text-color: var(--color-gray-90);
	--text-color-link: #082840;
	--text-color-link-active: #5f2b48;
	--text-color-link-visited: #17050F;

	--syntax-tab-size: 2;
}

@media (prefers-color-scheme: dark) {
	:root {
		--color-gray-20: #e0e0e0;
		--color-gray-50: #C0C0C0;
		--color-gray-90: #dad8d8;

		/* --text-color is assigned to --color-gray-_ above */
		--text-color-link: #1493fb;
		--text-color-link-active: #6969f7;
		--text-color-link-visited: #a6a6f8;

		--background-color: #15202b;
	}
}


/* Global stylesheet */
* {
	box-sizing: border-box;
}

html,
body {
	padding: 0;
	margin: 0 auto;
	font-family: var(--font-family);
	color: var(--text-color);
	background-color: var(--background-color);
}
html {
	overflow-y: scroll;
}
body {
	max-width: 50em;
}

/* https://www.a11yproject.com/posts/how-to-hide-content/ */
.visually-hidden {
	clip: rect(0 0 0 0);
	clip-path: inset(50%);
	height: 1px;
	overflow: hidden;
	position: absolute;
	white-space: nowrap;
	width: 1px;
}

p:last-child {
	margin-bottom: 0;
}
p {
	line-height: 1.5;
}

li {
	line-height: 1.5;
}

a[href] {
	color: var(--text-color-link);
}
a[href]:visited {
	color: var(--text-color-link-visited);
}
a[href]:hover,
a[href]:active {
	color: var(--text-color-link-active);
}

main {
	padding: 1rem;
}
main :first-child {
	margin-top: 0;
}

header {
	border-bottom: 1px dashed var(--color-gray-20);
}
header:after {
	content: "";
	display: table;
	clear: both;
}

.links-nextprev {
	list-style: none;
	border-top: 1px dashed var(--color-gray-20);
	padding: 1em 0;
}

table {
	margin: 1em 0;
}
table td,
table th {
	padding-right: 1em;
}

pre,
code {
	font-family: var(--font-family-monospace);
}
pre:not([class*="language-"]) {
	margin: .5em 0;
	line-height: 1.375; /* 22px /16 */
	-moz-tab-size: var(--syntax-tab-size);
	-o-tab-size: var(--syntax-tab-size);
	tab-size: var(--syntax-tab-size);
	-webkit-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
	direction: ltr;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
}
code {
	word-break: break-all;
}

/* Header */
header {
	display: flex;
	gap: 1em .5em;
	flex-wrap: wrap;
	align-items: center;
	padding: 1em;
}
.home-link {
	font-size: 1em; /* 16px /16 */
	font-weight: 700;
	margin-right: 2em;
}
.home-link:link:not(:hover) {
	text-decoration: none;
}

/* Nav */
.nav {
	display: flex;
	padding: 0;
	margin: 0;
	list-style: none;
}
.nav-item {
	display: inline-block;
	margin-right: 1em;
}
.nav-item a[href]:not(:hover) {
	text-decoration: none;
}
.nav a[href][aria-current="page"] {
	text-decoration: underline;
}

/* Posts list */
.postlist {
	list-style: none;
	padding: 0;
	padding-left: 1.5rem;
}
.postlist-item {
	display: flex;
	flex-wrap: wrap;
	align-items: baseline;
	counter-increment: start-from -1;
	margin-bottom: 1em;
}
.postlist-item:before {
	display: inline-block;
	pointer-events: none;
	content: "" counter(start-from, decimal-leading-zero) ". ";
	line-height: 100%;
	text-align: right;
	margin-left: -1.5rem;
}
.postlist-date,
.postlist-item:before {
	font-size: 0.8125em; /* 13px /16 */
	color: var(--color-gray-90);
}
.postlist-date {
	word-spacing: -0.5px;
}
.postlist-link {
	font-size: 1.1875em; /* 19px /16 */
	font-weight: 700;
	flex-basis: calc(100% - 1.5rem);
	padding-left: .25em;
	padding-right: .5em;
	text-underline-position: from-font;
	text-underline-offset: 0;
	text-decoration-thickness: 1px;
}
.postlist-item-active .postlist-link {
	font-weight: bold;
}

/* Tags */
.post-tag {
	display: inline-flex;
	align-items: center;
	justify-content: center;
	text-transform: capitalize;
	font-style: italic;
}
.postlist-item > .post-tag {
	align-self: center;
}

/* Tags list */
.post-metadata {
	display: inline-flex;
	flex-wrap: wrap;
	gap: .5em;
	list-style: none;
	padding: 0;
	margin: 0;
}
.post-metadata time {
	margin-right: 1em;
}

/* Direct Links / Markdown Headers */
.header-anchor {
	text-decoration: none;
	font-style: normal;
	font-size: 1em;
	margin-left: .1em;
}
a[href].header-anchor,
a[href].header-anchor:visited {
	color: transparent;
}
a[href].header-anchor:focus,
a[href].header-anchor:hover {
	text-decoration: underline;
}
a[href].header-anchor:focus,
:hover > a[href].header-anchor {
	color: #aaa;
}

h2 + .header-anchor {
	font-size: 1.5em;
}</style>
	</head>
	<body>
		<a href="#skip" class="visually-hidden">Skip to main content</a>

		<header>
			<a href="/" class="home-link">dctanner&#39;s blog</a>
			<nav>
				<h2 class="visually-hidden">Top level navigation menu</h2>
				<ul class="nav">
					<li class="nav-item"><a href="/">Writing</a></li>
					<li class="nav-item"><a href="/about/">About Me</a></li>
				</ul>
			</nav>
		</header>

		<main id="skip">
			
<h1>Building an AI SuperServer for LLM training and experiments</h1>

<ul class="post-metadata">
	<li><time datetime="2024-03-14">14 March 2024</time></li>
</ul>

<p>Impressive new language models like Llama and Mistral have broadened the accessibility of AI training. If you want to fine-tune a model with your own data, it's now relatively easy to do with tools like <a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a> and a few dollars spent on a GPU cloud. But if you want to go deeper and train larger models or try new methods, the cloud bill can quickly rack up. Renting 8 A100's on AWS will set you back an astounding $350,000 per year! There are cheaper clouds, but they can still cost tens of thousands a year.</p>
<p>I've always enjoyed building PCs. I remember when I was 16 and my grandma bought me my first PC to assemble myself. So in the name of fun and saving money, I embarked on building an AI server so that I can more affordably do independent AI research.</p>
<h1 id="your-options" tabindex="-1">Your options <a class="header-anchor" href="#your-options">#</a></h1>
<p>Depending on your budget and use case, there are a few routes to take when building an AI server.</p>
<h2 id="open-frame" tabindex="-1">Open frame <a class="header-anchor" href="#open-frame">#</a></h2>
<p><picture><source type="image/avif" srcset="/img/l6X6xCxgFa-2048.avif 2048w"><source type="image/webp" srcset="/img/l6X6xCxgFa-2048.webp 2048w"><img alt="Miner style" loading="lazy" decoding="async" style="width: 100%; height: auto;" src="/img/l6X6xCxgFa-2048.jpeg" width="2048" height="1536"></picture></p>
<p>If the server is just for you, and you want to keep it at home or in your basement, the most affordable option is essentially a powerful consumer PC, with an open frame case (originally designed for crypto miners). You'll be able to find a lots of advice on Reddit for this route.</p>
<p>The important things are a motherboard that has lots of 16x PCIe slots, PCIe risers with redrivers, and multiple PSUs (depending the number of GPUs you choose). You'll be able to buy everything second had if you like, including the GPUs. For GPUs you're best going with RTX 3090s or 4090s in this setup, and because there's no case, you won't have issues with space or airflow.</p>
<p>The benefit if this route is cost, but also the ability to start simple with just a single GPU and grow as you desire by adding more.</p>
<h2 id="rack-server" tabindex="-1">Rack server <a class="header-anchor" href="#rack-server">#</a></h2>
<p><picture><source type="image/avif" srcset="/img/U4DUeJHjp3-1280.avif 1280w"><source type="image/webp" srcset="/img/U4DUeJHjp3-1280.webp 1280w"><img alt="Server style" loading="lazy" decoding="async" style="width: 100%; height: auto;" src="/img/U4DUeJHjp3-1280.jpeg" width="1280" height="960"></picture></p>
<p>If you're planning to train larger models, have more servers, datacenter GPUs or just don't have anywhere to house a noisy hot server at home, you can go the rack mountable server route. This is the the route I've gone, as our house doesn't have a basement and our internet isn't that fast. My server now lives in a datacenter where it's cooled and well connected.</p>
<p>I found less resources on this route, so the rest of this guide is aimed at helping you build and setup a rack mountable GPU server.</p>
<h1 id="building-the-server" tabindex="-1">Building the server <a class="header-anchor" href="#building-the-server">#</a></h1>
<p>Supermicro make great server systems and many specifically for AI use cases. For example the <a href="https://www.supermicro.com/en/products/system/4u/4029/sys-4029gp-trt2.cfm">SuperServer 4029GP-TRT2</a> is a mid range 4U dual CPU server with 10 PCIe slots - ideal for filling with GPUs! I found a well priced one from an IT supplier in the UK. The newer model is more expensive, but may be easier to find. Note that the model I used only have PCIe 3.0. If you are using RTX 4090 or a newer datacenter GPU, you will probably want the newer model which supports PCIe 4.0.</p>
<p><picture><source type="image/avif" srcset="/img/rKkyq27rxN-960.avif 960w"><img alt="SuperServer 4029GP-TRT2" loading="lazy" decoding="async" style="width: 100%; height: auto;" src="/img/rKkyq27rxN-960.webp" width="960" height="720"></picture></p>
<p>It arrived at my house on a pallet. It was heavier than I expected!</p>
<p><picture><source type="image/avif" srcset="/img/x5qFkxTOGi-1280.avif 1280w"><source type="image/webp" srcset="/img/x5qFkxTOGi-1280.webp 1280w"><img alt="The pallet" loading="lazy" decoding="async" style="width: 100%; height: auto;" src="/img/x5qFkxTOGi-1280.jpeg" width="1280" height="960"></picture></p>
<p>After lugging it up the stairs and reading the manual, I installed 10 RTX 3090s I bought second hand from someone who previously was using them for mining. Note that to fit the maximum number of GPUs in a system you'll need to find blower or turbo style GPUs that are only two slots wide. The vast majority of 3090 and 4090 GPUs are for gaming, and they will take up 3 slots and the power comes out the top and you won't be able to put the case on your server. If you can't find blower consumer GPUs, you're next best bet is the RTX A6000 which is still fairly good value for money, even if it's still 3x more than a 4090.</p>
<p>You'll also need to add the CPUs (two of them), memory and storage. I sourced everything secondhand from eBay. Most things cost no more than a few hundred dollars each. I went with 2x Intel Xeon Platinum 8160, 12x32GB DDR memory and an 8TB SSD.</p>
<p>Once everything was installed, I turned it on for the first time - what I heard could only be described as a mini jet engine. Server fans are noisy.</p>
<p>Next step was to setup the OS and environment.</p>
<h1 id="setting-up-the-os" tabindex="-1">Setting up the OS <a class="header-anchor" href="#setting-up-the-os">#</a></h1>
<p>Supermicro servers have in inbuilt webui called IPMI for accessing the server console and monitor output. There is a dedicated lan port for PICE on this server. You should also plug in a second lan cable to one of the main lan ports, otherwise your server won't actually have internet access (this confused me initially).</p>
<p>It will find an IP with DHCP, so I just logged into my router to see the IP it was assigned and visited that in my browser. You'll be asked to login, username is 'ADMIN' and the password is printed stickers in several places in your server case.</p>
<p>I decided to install Ubuntu 22.04 sever. Create a bootable Ubuntu USB stick and plug it into the server. Now connect to the webui console by going to the server's IP then clicking Remote Control &gt; iKVM/HTML5 and click the button. You can now reboot the server and you'll see the BIOS popup, where you can hit an F key to choose a boot drive. Do this and select the USB.</p>
<p>The IPMI web console doesn't support pasting text. So getting your ssh pubkey over is a bit of a pain. Here's a solution I've used:</p>
<ol>
<li>On your local computer with has your ssh pubkey on it, run <code>cd .ssh &amp;&amp; python -m http.server</code> (you are about to serve your private key over http without authentication, please be aware this isn't a great idea).</li>
<li>On the server, via the IPMI web console, login with the user you created when installing Ubuntu, and run <code>wget -qO - &quot;http://192.168.178.21:8000/id_ed25519.pub&quot; &gt; ~/.ssh/authorized_keys &amp;&amp; chmod 600 .ssh/authorized_keys</code>.</li>
<li>You should now be able to ssh into your server. Remember to stop the <code>python -m http.server</code> on your local computer now.</li>
</ol>
<h1 id="important-system-tweaks" tabindex="-1">Important system tweaks <a class="header-anchor" href="#important-system-tweaks">#</a></h1>
<p>There are some tweaks we can do to improve the performance and reliability of our server. Following <a href="https://towardsdatascience.com/deploying-kubeflow-to-a-bare-metal-gpu-cluster-from-scratch-6865ebcde032">the tips here</a> (<a href="https://archive.ph/0Y2DK#selection-611.0-611.103">archived page</a> if Medium paywalls that page), first disable the kernel security patches on computing instances. The collateral performance penalty is much more expensive than the imposed risks. Edit /etc/default/grub and add:</p>
<pre><code>GRUB_CMDLINE_LINUX_DEFAULT=&quot;pti=off spectre_v2=off l1tf=off nospec_store_bypass_disable no_stf_barrier&quot;
</code></pre>
<p>It's also critical to disable IOMMU if you plan peer-to-peer GPU communication, e.g., multi-GPU model training in Tensorflow or PyTorch. Also add to /etc/default/grub:</p>
<pre><code>GRUB_CMDLINE_LINUX_DEFAULT=&quot;intel_iommu=off rcutree.rcu_idle_gp_delay=1&quot;
</code></pre>
<h1 id="check-gpu-p2p-communication" tabindex="-1">Check GPU P2P communication <a class="header-anchor" href="#check-gpu-p2p-communication">#</a></h1>
<p>If you're using a GPU that supports it, P2P communication speeds up things a lot.</p>
<p>Note it's important check <a href="https://docs.nvidia.com/deeplearning/nccl/archives/nccl_284/user-guide/docs/troubleshooting.html#:~:text=PCI%20Access%20Control%20Services%20(ACS)%C2%B6&amp;text=If%20PCI%20switches%20have%20ACS,done%20again%20after%25z">PCI Access Control Services (ACS)</a> is disabled.</p>
<p>You can follow these steps to test your system's GPU P2P speed: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#gpu-to-gpu-communication</p>
<h1 id="nvidia-drivers-and-python-environment" tabindex="-1">NVIDIA drivers and python environment <a class="header-anchor" href="#nvidia-drivers-and-python-environment">#</a></h1>
<p>We now want to get the NVIDIA drivers, CUDA and our Python envs setup.</p>
<p>I've had success using these steps to install CUDA v11.8: https://gist.github.com/MihailCosmin/affa6b1b71b43787e9228c25fe15aeba
Some people have mentioned using a higher NVIDIA drivers version than the nvidia-driver-515 in the script. But be beware there's a bug in driver version 545 that prevents 3090 and 4090 cards from using P2P (see <a href="https://github.com/NVIDIA/nccl-tests/issues/117">this github issue</a> for a discussion on the problem). If you have a driver with this bug, you may find your training run stalls and times out. Version 535 worked well for me.</p>
<p>I like to use Conda with the <a href="https://www.fast.ai/posts/2021-07-15-fastconda.html">fastchan channel</a> for my environments. But you may enjoy a different python virtual env tool.</p>
<h1 id="now-you-can-train-some-ai" tabindex="-1">Now you can train some AI <a class="header-anchor" href="#now-you-can-train-some-ai">#</a></h1>
<p><picture><source type="image/avif" srcset="/img/WfDXmhx_Ty-1482.avif 1482w"><source type="image/webp" srcset="/img/WfDXmhx_Ty-1482.webp 1482w"><img alt="nvidia-smi" loading="lazy" decoding="async" style="width: 100%; height: auto;" src="/img/WfDXmhx_Ty-1482.jpeg" width="1482" height="1338"></picture></p>
<p>I'm enjoying using <a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a> for LLM fine tuning. <a href="https://huggingface.co/docs/transformers/index">HuggingFace Transformers</a> is also a great place to start.</p>
<h1 id="datacenter-trip" tabindex="-1">Datacenter trip <a class="header-anchor" href="#datacenter-trip">#</a></h1>
<p><picture><source type="image/avif" srcset="/img/Fd4oi2rcxB-1922.avif 1922w"><source type="image/webp" srcset="/img/Fd4oi2rcxB-1922.webp 1922w"><img alt="Datacenter" loading="lazy" decoding="async" style="width: 100%; height: auto;" src="/img/Fd4oi2rcxB-1922.png" width="1922" height="1294"></picture></p>
<p>Since the GPUs are super noisy and hot, I found a local datacenter that would colocate it for a reasonable cost. Installation was easier than I expected, although we ended up putting it on a lower slot on the rack because it was too heavy to lift half way up without a lift.</p>
<p>This <a href="https://www.datacate.net/wp-content/uploads/2019/04/Colocation-Survival-Guide-6x9-with-bonus-material.pdf">Colocation Survival Guide</a> was super helpful, as it walks you through all the aspects of colocating, from the physical setup to networking.</p>
<h1 id="other-things" tabindex="-1">Other things <a class="header-anchor" href="#other-things">#</a></h1>
<h2 id="set-a-lower-max-power-limit-for-gpus" tabindex="-1">Set a lower max power limit for GPUs <a class="header-anchor" href="#set-a-lower-max-power-limit-for-gpus">#</a></h2>
<p>Some people find that lowering the power limit just a bit will reduce max temp without any real performance sacrifice. I set the max power for my RTX 3090's to 300W (from 305W) by <a href="https://www.reddit.com/r/Fedora/comments/11lh9nn/set_nvidia_gpu_power_and_temp_limit_on_boot/">following these steps</a>.</p>
<h2 id="docker-bug-workaround" tabindex="-1">Docker bug workaround <a class="header-anchor" href="#docker-bug-workaround">#</a></h2>
<p>If you're planning to use Docker with the GPUs, note there's <a href="https://github.com/NVIDIA/nvidia-container-toolkit/issues/48">a bug on Ubuntu 22.04 which needs working around</a>.</p>
<h1 id="going-bigger" tabindex="-1">Going bigger? <a class="header-anchor" href="#going-bigger">#</a></h1>
<p>If you're planing to build a cluster, there is an excellent video from the Lambda team: <a href="https://www.youtube.com/watch?v=rfu5FwncZ6s">Building a GPU cluster for AI</a>.</p>

<ul class="links-nextprev"><li>Next: <a href="/blog/your-rag-may-not-need-a-vector-store/">Using LLM tool calling and long context for better RAG</a></li>
</ul>

		</main>

		<footer></footer>

		<!-- This page `/blog/building-an-ai-superserver/` was built on 2024-04-25T11:15:07.909Z -->
	</body>
</html>
